"""
Publication-Ready Results Export Module

This module provides functions to export experimental results with confidence intervals
in formats suitable for scientific publications (LaTeX, CSV, Markdown).

Author: LoRaChat Testing Framework
Date: 2025
"""

import os
import json
import csv
from typing import Dict, List, Optional
from datetime import datetime
import numpy as np


def export_to_latex(results: List[Dict], output_path: str, table_caption: str = "Experimental Results",
                    table_label: str = "tab:results", confidence_level: float = 0.95) -> str:
    """
    Export results to a LaTeX table suitable for scientific papers.

    Args:
        results: List of result dictionaries with 'label', 'n', and 'metrics'
        output_path: Path to save the .tex file
        table_caption: Caption for the table
        table_label: LaTeX label for referencing
        confidence_level: Confidence level used (for caption)

    Returns:
        Path to the created LaTeX file
    """
    ci_pct = int(confidence_level * 100)

    # Start building LaTeX content
    latex_content = []
    latex_content.append("% LaTeX table generated by LoRaChat Testing Framework")
    latex_content.append(f"% Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    latex_content.append("")
    latex_content.append("\\begin{table}[htbp]")
    latex_content.append("\\centering")
    latex_content.append(f"\\caption{{{table_caption}}}")
    latex_content.append(f"\\label{{{table_label}}}")

    # Determine which metrics are present across all results
    all_metrics = set()
    for result in results:
        if result.get('metrics'):
            all_metrics.update(result['metrics'].keys())

    metric_names = sorted(list(all_metrics))

    # Build table header
    n_cols = 1 + len(metric_names) * 3 + 1  # Config + (Mean, SD, CI) per metric + n
    latex_content.append("\\begin{tabular}{l" + "c" * (n_cols - 1) + "}")
    latex_content.append("\\hline")

    # Header row 1: Configuration and Metric names (spanning columns)
    header1 = "Configuration"
    for metric_name in metric_names:
        # Clean metric name for display
        display_name = metric_name.replace("_", " ")
        header1 += f" & \\multicolumn{{3}}{{c}}{{{display_name}}}"
    header1 += " & $n$ \\\\"
    latex_content.append(header1)

    # Header row 2: Mean, SD, CI for each metric
    header2 = ""
    for _ in metric_names:
        header2 += " & Mean (\\%) & SD (\\%) & {CI_pct}\\% CI (\\%)"
    header2 += " & \\\\"
    latex_content.append(header2)
    latex_content.append("\\hline")

    # Data rows
    for result in results:
        config_label = result['label'].replace("_", "\\_")  # Escape underscores
        row = config_label

        for metric_name in metric_names:
            metric_data = result['metrics'].get(metric_name)

            if metric_data is None:
                # Metric not available for this configuration
                row += " & --- & --- & ---"
            elif isinstance(metric_data, dict):
                # Experimental metric with CI
                mean = metric_data['mean']
                std = metric_data['std']
                ci_lower = metric_data['ci_lower']
                ci_upper = metric_data['ci_upper']
                row += f" & {mean:.2f} & {std:.2f} & [{ci_lower:.2f}, {ci_upper:.2f}]"
            else:
                # Theoretical value (single number)
                row += f" & {metric_data:.2f} & --- & ---"

        row += f" & {result['n']} \\\\"
        latex_content.append(row)

    latex_content.append("\\hline")
    latex_content.append("\\end{tabular}")
    latex_content.append("\\end{table}")
    latex_content.append("")
    latex_content.append(f"% Note: CI = Confidence Interval at {ci_pct}\\% level using Student's t-distribution")

    # Write to file
    with open(output_path, 'w') as f:
        f.write('\n'.join(latex_content))

    return output_path


def export_to_csv(results: List[Dict], output_path: str, confidence_level: float = 0.95) -> str:
    """
    Export results to CSV format for import into spreadsheets or statistical software.

    Args:
        results: List of result dictionaries
        output_path: Path to save the .csv file
        confidence_level: Confidence level used

    Returns:
        Path to the created CSV file
    """
    # Collect all unique metrics
    all_metrics = set()
    for result in results:
        if result.get('metrics'):
            all_metrics.update(result['metrics'].keys())

    metric_names = sorted(list(all_metrics))

    # Build CSV header
    header = ['Configuration', 'n']
    for metric_name in metric_names:
        header.extend([
            f'{metric_name}_Mean',
            f'{metric_name}_SD',
            f'{metric_name}_CI_Lower',
            f'{metric_name}_CI_Upper',
            f'{metric_name}_Median',
            f'{metric_name}_Values'
        ])

    # Build data rows
    rows = []
    for result in results:
        row = {
            'Configuration': result['label'],
            'n': result['n']
        }

        for metric_name in metric_names:
            metric_data = result['metrics'].get(metric_name)

            if metric_data is None:
                row[f'{metric_name}_Mean'] = ''
                row[f'{metric_name}_SD'] = ''
                row[f'{metric_name}_CI_Lower'] = ''
                row[f'{metric_name}_CI_Upper'] = ''
                row[f'{metric_name}_Median'] = ''
                row[f'{metric_name}_Values'] = ''
            elif isinstance(metric_data, dict):
                row[f'{metric_name}_Mean'] = metric_data['mean']
                row[f'{metric_name}_SD'] = metric_data['std']
                row[f'{metric_name}_CI_Lower'] = metric_data['ci_lower']
                row[f'{metric_name}_CI_Upper'] = metric_data['ci_upper']
                row[f'{metric_name}_Median'] = np.median(metric_data['values']) if 'values' in metric_data else ''
                row[f'{metric_name}_Values'] = ';'.join(map(str, metric_data['values'])) if 'values' in metric_data else ''
            else:
                # Theoretical value
                row[f'{metric_name}_Mean'] = metric_data
                row[f'{metric_name}_SD'] = ''
                row[f'{metric_name}_CI_Lower'] = ''
                row[f'{metric_name}_CI_Upper'] = ''
                row[f'{metric_name}_Median'] = ''
                row[f'{metric_name}_Values'] = ''

        rows.append(row)

    # Write to CSV
    with open(output_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        writer.writerows(rows)

    return output_path


def export_to_markdown(results: List[Dict], output_path: str, confidence_level: float = 0.95) -> str:
    """
    Export results to Markdown table format.

    Args:
        results: List of result dictionaries
        output_path: Path to save the .md file
        confidence_level: Confidence level used

    Returns:
        Path to the created Markdown file
    """
    ci_pct = int(confidence_level * 100)

    # Collect all unique metrics
    all_metrics = set()
    for result in results:
        if result.get('metrics'):
            all_metrics.update(result['metrics'].keys())

    metric_names = sorted(list(all_metrics))

    # Start building markdown content
    md_content = []
    md_content.append(f"# Experimental Results\n")
    md_content.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    md_content.append(f"**Confidence Level:** {ci_pct}%\n")

    # Build table header
    header = "| Configuration | n |"
    separator = "|---------------|---|"

    for metric_name in metric_names:
        display_name = metric_name.replace("_", " ")
        header += f" {display_name} (Mean ± SD) | {ci_pct}% CI |"
        separator += "----------------------|-----------|"

    md_content.append(header)
    md_content.append(separator)

    # Build data rows
    for result in results:
        row = f"| {result['label']} | {result['n']} |"

        for metric_name in metric_names:
            metric_data = result['metrics'].get(metric_name)

            if metric_data is None:
                row += " --- | --- |"
            elif isinstance(metric_data, dict):
                mean = metric_data['mean']
                std = metric_data['std']
                ci_lower = metric_data['ci_lower']
                ci_upper = metric_data['ci_upper']
                row += f" {mean:.2f} ± {std:.2f}% | [{ci_lower:.2f}, {ci_upper:.2f}]% |"
            else:
                # Theoretical value
                row += f" {metric_data:.2f}% (theoretical) | --- |"

        md_content.append(row)

    md_content.append("")
    md_content.append(f"*CI = Confidence Interval calculated using Student's t-distribution*")

    # Write to file
    with open(output_path, 'w') as f:
        f.write('\n'.join(md_content))

    return output_path


def generate_methods_section(sample_sizes: List[int], confidence_level: float = 0.95,
                             includes_normality_test: bool = True,
                             includes_bootstrap: bool = False) -> str:
    """
    Generate a Methods section text describing the statistical analysis.

    Args:
        sample_sizes: List of sample sizes used (will report range)
        confidence_level: Confidence level used
        includes_normality_test: Whether normality testing was performed
        includes_bootstrap: Whether bootstrap CI was calculated

    Returns:
        Formatted text suitable for a Methods section
    """
    ci_pct = int(confidence_level * 100)
    n_min = min(sample_sizes)
    n_max = max(sample_sizes)

    methods_text = f"""## Statistical Analysis

Each experimental configuration was repeated {n_min}"""

    if n_min != n_max:
        methods_text += f" to {n_max}"

    methods_text += f""" times. Results are reported as mean ± standard deviation (SD) unless otherwise noted.

Confidence intervals ({ci_pct}% CI) were calculated using Student's t-distribution with n-1 degrees of freedom, which is appropriate for small sample sizes [1]. The confidence interval for the mean was computed as:

    CI = mean ± t(α/2, n-1) × (SD / √n)

where t(α/2, n-1) is the critical value from the t-distribution with n-1 degrees of freedom at significance level α = {1 - confidence_level:.2f}."""

    if includes_normality_test:
        methods_text += """\n\nNormality of the data was assessed using the Shapiro-Wilk test [2]. """
        if n_min < 8:
            methods_text += f"""Due to the small sample sizes (n = {n_min}"""
            if n_min != n_max:
                methods_text += f" to {n_max}"
            methods_text += """), the statistical power of normality tests is limited, and results should be interpreted cautiously."""

    if includes_bootstrap:
        methods_text += """\n\nAs a sensitivity analysis, non-parametric bootstrap confidence intervals were also calculated using 10,000 bootstrap resamples with the percentile method [3]. Bootstrap CIs provide a distribution-free alternative that does not assume normality."""

    if n_min < 5:
        methods_text += f"""\n\nGiven the small sample sizes (n = {n_min}"""
        if n_min != n_max:
            methods_text += f" to {n_max}"
        methods_text += """), individual data points are reported alongside summary statistics to provide transparency about the underlying data distribution. The wide confidence intervals appropriately reflect both measurement variability and statistical uncertainty due to limited sample size."""

    methods_text += """\n\n### References

[1] Student (1908). "The probable error of a mean". Biometrika, 6(1), 1-25.

[2] Shapiro, S. S., & Wilk, M. B. (1965). "An analysis of variance test for normality (complete samples)". Biometrika, 52(3-4), 591-611."""

    if includes_bootstrap:
        methods_text += """\n\n[3] Efron, B., & Tibshirani, R. J. (1994). "An Introduction to the Bootstrap". CRC press."""

    return methods_text


def generate_summary_statistics(results: List[Dict], output_path: Optional[str] = None) -> str:
    """
    Generate a comprehensive text summary of all statistics.

    Args:
        results: List of result dictionaries
        output_path: Optional path to save summary as text file

    Returns:
        Formatted summary text
    """
    summary_lines = []
    summary_lines.append("="*70)
    summary_lines.append("STATISTICAL SUMMARY - EXPERIMENTAL RESULTS")
    summary_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_lines.append("="*70)
    summary_lines.append("")

    for result in results:
        summary_lines.append(f"\nConfiguration: {result['label']}")
        summary_lines.append(f"Sample Size: n = {result['n']}")
        summary_lines.append("-"*70)

        metrics = result.get('metrics', {})

        for metric_name, metric_data in metrics.items():
            summary_lines.append(f"\n  {metric_name}:")

            if isinstance(metric_data, dict):
                # Experimental metric with full statistics
                summary_lines.append(f"    Mean:           {metric_data['mean']:.4f}%")
                summary_lines.append(f"    Median:         {np.median(metric_data['values']) if 'values' in metric_data else 'N/A':.4f}%")
                summary_lines.append(f"    SD:             {metric_data['std']:.4f}%")
                summary_lines.append(f"    SE:             {metric_data.get('se', 'N/A') if isinstance(metric_data.get('se'), (int, float)) else 'N/A'}%")
                summary_lines.append(f"    95% CI:         [{metric_data['ci_lower']:.4f}, {metric_data['ci_upper']:.4f}]%")
                summary_lines.append(f"    CI Width:       {metric_data['ci_upper'] - metric_data['ci_lower']:.4f}%")

                if 'values' in metric_data and len(metric_data['values']) > 0:
                    values_str = ', '.join([f"{v:.2f}" for v in metric_data['values']])
                    summary_lines.append(f"    Individual values: [{values_str}]%")
                    summary_lines.append(f"    Range:          [{min(metric_data['values']):.4f}, {max(metric_data['values']):.4f}]%")
            else:
                # Theoretical value
                summary_lines.append(f"    Value (Theoretical): {metric_data:.4f}%")

        summary_lines.append("")

    summary_lines.append("="*70)
    summary_lines.append("END OF SUMMARY")
    summary_lines.append("="*70)

    summary_text = '\n'.join(summary_lines)

    # Save to file if path provided
    if output_path:
        with open(output_path, 'w') as f:
            f.write(summary_text)

    return summary_text


def export_all_formats(results: List[Dict], output_dir: str, base_filename: str = "results",
                      confidence_level: float = 0.95, table_caption: str = "Experimental Results") -> Dict[str, str]:
    """
    Export results in all formats (LaTeX, CSV, Markdown, Summary).

    Args:
        results: List of result dictionaries
        output_dir: Directory to save all output files
        base_filename: Base filename (without extension)
        confidence_level: Confidence level used
        table_caption: Caption for LaTeX table

    Returns:
        Dictionary mapping format names to file paths
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    output_files = {}

    # Export to LaTeX
    latex_path = os.path.join(output_dir, f"{base_filename}.tex")
    export_to_latex(results, latex_path, table_caption, confidence_level=confidence_level)
    output_files['latex'] = latex_path

    # Export to CSV
    csv_path = os.path.join(output_dir, f"{base_filename}.csv")
    export_to_csv(results, csv_path, confidence_level=confidence_level)
    output_files['csv'] = csv_path

    # Export to Markdown
    md_path = os.path.join(output_dir, f"{base_filename}.md")
    export_to_markdown(results, md_path, confidence_level=confidence_level)
    output_files['markdown'] = md_path

    # Generate summary
    summary_path = os.path.join(output_dir, f"{base_filename}_summary.txt")
    generate_summary_statistics(results, summary_path)
    output_files['summary'] = summary_path

    # Generate methods section
    sample_sizes = [r['n'] for r in results]
    methods_path = os.path.join(output_dir, f"{base_filename}_methods.txt")
    methods_text = generate_methods_section(sample_sizes, confidence_level)
    with open(methods_path, 'w') as f:
        f.write(methods_text)
    output_files['methods'] = methods_path

    return output_files


def print_export_summary(output_files: Dict[str, str]):
    """
    Print a summary of exported files to console.

    Args:
        output_files: Dictionary of format names to file paths
    """
    print("\n" + "="*70)
    print("EXPORT COMPLETE - Publication-Ready Files Generated")
    print("="*70)

    for format_name, file_path in output_files.items():
        print(f"  {format_name.upper():15} -> {file_path}")

    print("="*70)
    print("\nNext steps:")
    print("  1. Review the summary.txt for complete statistics")
    print("  2. Copy the methods.txt content to your paper's Methods section")
    print("  3. Import the .tex file into your LaTeX document")
    print("  4. Use the .csv file for further statistical analysis")
    print("  5. Use the .md file for README or documentation")
    print("="*70 + "\n")
